{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09218569-5357-4696-9f42-e6cb2e8128f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Model sentence-transformers/all-mpnet-base-v2 is currently loading', 'estimated_time': 20.0}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def query(payload, model_id, api_token):\n",
    "\theaders = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "\tAPI_URL = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "model_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "api_token = \"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\" # get yours at hf.co/settings/tokens\n",
    "HUGGINGFACE_API_KEY = api_token\n",
    "search_str = \"The goal of life is [MASK].\"\n",
    "data = query(search_str, model_id, api_token)\n",
    "print(data)\n",
    "\n",
    "# headers = {\n",
    "#     'Authorization': f\"Bearer {HUGGINGFACE_API_KEY}\",\n",
    "# }\n",
    "# response = requests.post(f\"https://api-inference.huggingface.co/models/{model_id}\", headers=headers, json=search_str)\n",
    "# if response.status_code == 200:\n",
    "#     vector = response.json()\n",
    "# else:\n",
    "#     print(headers)\n",
    "#     raise requests.exceptions.HTTPError(response.status_code, response.reason)\n",
    "\n",
    "# print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d91c621-9295-436d-9f00-c05b17e517c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from numba.core.errors import NumbaDeprecationWarning\n",
    "import warnings\n",
    "# Ignore Numba Deprecation Warnings\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "\n",
    "import json\n",
    "import multiprocessing\n",
    "import mysql.connector\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import re\n",
    "import torch\n",
    "import transformers\n",
    "import umap\n",
    "import weaviate\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "from datetime import date, datetime\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "from database_topic_modeling.utils.language_dependant_utils import (\n",
    "    get_top_n_titles_per_class, \n",
    "    language_representation,\n",
    "    retrieve_top_n_words_per_class,\n",
    "    tokenization_and_stopword_removal,\n",
    ")\n",
    "from database_topic_modeling.utils.utils import (\n",
    "    clustering,\n",
    "    create_tokens_per_class_label_and_vocab,\n",
    "    dim_reduction_3d,\n",
    "    extract_articles_per_date,\n",
    "    get_sentence_summaries,\n",
    "    plot_dendrogram,\n",
    "    scatter_3d,\n",
    "    tf_idf_matrix,\n",
    ")\n",
    "from database_topic_modeling.utils.weaviate_utils import (\n",
    "    get_nearest_oldids_without_neighbors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cdf65db-c0f2-4817-afba-a50a986875ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_str = \"North Korea\"\n",
    "weaviate_url=\"http://192.168.20.101:8080\"\n",
    "weaviate_auth_api_key=\"2d1c4cc7-7175-4965-936f-5439695e9c65\"\n",
    "class_name=\"Openalex_cn_new\"\n",
    "id_column_name=\"id\"\n",
    "language='en'\n",
    "sentence_transformer_name ='all-mpnet-base-v2'\n",
    "num_nearest_neighbors =500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6738d570-b350-46e2-bd0e-6b8c82d670a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████| 500/500 [00:00<00:00, 27670.20 examples/s]\n",
      "Map: 100%|████████████████████████████| 500/500 [00:00<00:00, 19717.86 examples/s]\n",
      "Map: 100%|████████████████████████████| 500/500 [00:00<00:00, 14708.50 examples/s]\n",
      "Map: 100%|████████████████████████████| 500/500 [00:00<00:00, 14850.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "search_str = language_representation(search_str, language)\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url=weaviate_url,\n",
    "    auth_client_secret=weaviate.AuthApiKey(api_key=weaviate_auth_api_key),\n",
    ")\n",
    "\n",
    "# Get all vectors from weaviate and create an embedding matrix and a full dataset from it\n",
    "small_embeds, small_dataset = get_nearest_oldids_without_neighbors(\n",
    "    sentence_transformer_name, \n",
    "    search_str, \n",
    "    client, \n",
    "    class_name, \n",
    "    id_column_name=id_column_name, \n",
    "    num_nearest_neighbors=num_nearest_neighbors\n",
    ")\n",
    "\n",
    "# umap to decrease dimensionality of the datapoints\n",
    "small_u = dim_reduction_3d('TSNE', small_embeds, n_components=3, perplexity=30.0, metric=\"cosine\", random_state=42)\n",
    "\n",
    "n_clusters = 10\n",
    "\n",
    "# agglomerative heirarchical clustering of the datapoints\n",
    "clusterer = clustering('agglomerative', small_u, n_clusters=n_clusters, metric='euclidean', linkage='ward')\n",
    "\n",
    "# add the cluster labels to our dataset\n",
    "small_dataset = small_dataset.add_column('class', clusterer.labels_)\n",
    "\n",
    "small_dataset = tokenization_and_stopword_removal(\n",
    "    small_dataset, \n",
    "    language=language, \n",
    "    column_to_tokenize=\"abstract\" # TODO: change back to sentencetext\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ae16c40-df05-4fba-9410-791f80744af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 6592/6592 [00:00<00:00, 2543124.72it/s]\n",
      "100%|████████████████████████████████████| 6592/6592 [00:00<00:00, 2579183.95it/s]\n",
      "100%|████████████████████████████████████| 6592/6592 [00:00<00:00, 2721611.57it/s]\n",
      "100%|████████████████████████████████████| 6592/6592 [00:00<00:00, 3226237.10it/s]\n",
      "100%|████████████████████████████████████| 6592/6592 [00:00<00:00, 3094789.79it/s]\n",
      "100%|████████████████████████████████████| 6592/6592 [00:00<00:00, 3077565.89it/s]\n",
      "100%|████████████████████████████████████| 6592/6592 [00:00<00:00, 3255487.10it/s]\n",
      "100%|████████████████████████████████████| 6592/6592 [00:00<00:00, 3315607.62it/s]\n",
      "100%|████████████████████████████████████| 6592/6592 [00:00<00:00, 3594494.54it/s]\n",
      "100%|████████████████████████████████████| 6592/6592 [00:00<00:00, 3598236.85it/s]\n",
      "100%|█████████████████████████████████████| 6592/6592 [00:00<00:00, 789764.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 top 6 terms: ['north', 'states', 'penalty', 'death', '’', 'china']\n",
      "Class 1 top 6 terms: ['belt', 'asia', 'geopolitical', 'international', 'risk', 'nuclear']\n",
      "Class 2 top 6 terms: ['china', '’', 'yunnan', 'hotel', 'lisu', 'province']\n",
      "Class 3 top 6 terms: ['china', 'COVID19', 'health', 'countries', '2020', 'scholar']\n",
      "Class 4 top 6 terms: ['republic', 'austria', 'eastern', 'largest', 'germany', 'europe']\n",
      "Class 5 top 6 terms: ['1971', 'channel', 'enormously', 'achieving', 'opponents', 'en']\n",
      "Class 6 top 6 terms: ['peninsula', 'korean', 'economic', 'china', 'north', 'korea']\n",
      "Class 7 top 6 terms: ['role', 'US', 'security', '’', 'djibouti', 'conflict']\n",
      "Class 8 top 6 terms: ['”', 'protection', 'global', 'human', '“', 'international']\n",
      "Class 9 top 6 terms: ['’', 'korea', 'north', 'south', 'korean', 'kim']\n",
      "{'china': 280, '’': 264, 'country': 213, 'korea': 203, 'COVID19': 181, 'north': 142, 'economic': 133, 'health': 119, 'south': 117, 'global': 106, '2020': 99, 'scholar': 99, '”': 99, '“': 99, 'korean': 86, 'state': 81, 'international': 81, 'nuclear': 64, 'security': 62, 'U': 53, 'asium': 52, 'en': 48, 'human': 45, 'risk': 44, 'peninsula': 36, 'geopolitical': 34, 'role': 33, 'europe': 32, 'death': 30, 'republic': 27, 'germany': 27, 'kim': 25, 'penalty': 23, 'belt': 22, 'conflict': 16, 'province': 13, 'djiboutus': 9, 'eastern': 7, 'protection': 7, 'largest': 6, 'austrium': 5, 'channel': 3, 'yunnan': 2, 'hotel': 2, 'lisu': 2, 'achieving': 2, '1971': 1, 'enormously': 1, 'opponent': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "classes, vocab = create_tokens_per_class_label_and_vocab(small_dataset)\n",
    "\n",
    "term_index_dict, tf, tf_idf = tf_idf_matrix(classes, vocab)\n",
    "\n",
    "labels = retrieve_top_n_words_per_class(tf_idf, classes.keys(), list(vocab), 6, language=language)\n",
    "\n",
    "word_cloud_json_1 = {}\n",
    "\n",
    "for v in labels.values():\n",
    "    list_words = ast.literal_eval(v)\n",
    "    for word in list_words:\n",
    "        word_cloud_json_1[word] = round(tf[:, term_index_dict[word]].sum())\n",
    "\n",
    "word_cloud_json_2 = {}\n",
    "\n",
    "for k, v in sorted(word_cloud_json.items(), key=lambda x:x[1], reverse=True):\n",
    "    singular = Word(k).singularize()\n",
    "    if singular in word_cloud_json_2:\n",
    "        word_cloud_json_2[singular] += v\n",
    "    else:\n",
    "        word_cloud_json_2[singular] = v\n",
    "\n",
    "print(word_cloud_json_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77b22a80-cfdb-49c1-983e-0fc777e27eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel, HdpModel\n",
    "from gensim import corpora\n",
    "\n",
    "corpus = \"\"\n",
    "\n",
    "for sent in small_dataset[\"abstract\"]:\n",
    "    corpus += sent\n",
    "\n",
    "corpus = corpus.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c967848-e023-4039-ac6e-f459581f5fa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m----> 5\u001b[0m dirichlet_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcorpora\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDictionary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m bow_corpus \u001b[38;5;241m=\u001b[39m [dirichlet_dict\u001b[38;5;241m.\u001b[39mdoc2bow(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m corpus]\n\u001b[1;32m      8\u001b[0m num_topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gensim/corpora/dictionary.py:78\u001b[0m, in \u001b[0;36mDictionary.__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_nnz \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprune_at\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprune_at\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     81\u001b[0m         msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents (total \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m corpus positions)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     82\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gensim/corpora/dictionary.py:204\u001b[0m, in \u001b[0;36mDictionary.add_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    201\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madding document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, docno, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# update Dictionary with the document\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc2bow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ignore the result, here we only care about updating token ids\u001b[39;00m\n\u001b[1;32m    206\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilt \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m documents (total \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m corpus positions)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_docs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pos)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gensim/corpora/dictionary.py:241\u001b[0m, in \u001b[0;36mDictionary.doc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert `document` into the bag-of-words (BoW) format = list of `(token_id, token_count)` tuples.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(document, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc2bow expects an array of unicode tokens on input, not a single string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Construct (word, frequency) mapping.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m counter \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "dirichlet_dict = corpora.Dictionary(corpus)\n",
    "bow_corpus = [dirichlet_dict.doc2bow(text) for text in corpus]\n",
    "\n",
    "num_topics = 10\n",
    "num_keywords = 300\n",
    "\n",
    "dirichlet_model = LdaModel(corpus=bow_corpus,\n",
    "                           id2word=dirichlet_dict,\n",
    "                           num_topics=num_topics,\n",
    "                           update_every=1,\n",
    "                           chunksize=len(bow_corpus),\n",
    "                           passes=20,\n",
    "                           alpha='auto')\n",
    "\n",
    "def order_subset_by_coherence(dirichlet_model, bow_corpus, num_topics=10, num_keywords=10):\n",
    "    \"\"\"\n",
    "    Orders topics based on their average coherence across the corpus\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        dirichlet_model : gensim.models.type_of_model\n",
    "        bow_corpus : list of lists (contains (id, freq) tuples)\n",
    "        num_topics : int (default=10)\n",
    "        num_keywords : int (default=10)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        ordered_topics, ordered_topic_averages: list of lists and list\n",
    "    \"\"\"\n",
    "    if type(dirichlet_model) == gensim.models.ldamodel.LdaModel:\n",
    "        shown_topics = dirichlet_model.show_topics(num_topics=num_topics, \n",
    "                                                   num_words=num_keywords,\n",
    "                                                   formatted=False)\n",
    "    elif type(dirichlet_model)  == gensim.models.hdpmodel.HdpModel:\n",
    "        shown_topics = dirichlet_model.show_topics(num_topics=150, # return all topics\n",
    "                                                   num_words=num_keywords,\n",
    "                                                   formatted=False)\n",
    "    model_topics = [[word[0] for word in topic[1]] for topic in shown_topics]\n",
    "    topic_corpus = dirichlet_model.__getitem__(bow=bow_corpus, eps=0) # cutoff probability to 0 \n",
    "\n",
    "    topics_per_response = [response for response in topic_corpus]\n",
    "    flat_topic_coherences = [item for sublist in topics_per_response for item in sublist]\n",
    "\n",
    "    significant_topics = list(set([t_c[0] for t_c in flat_topic_coherences])) # those that appear\n",
    "    topic_averages = [sum([t_c[1] for t_c in flat_topic_coherences if t_c[0] == topic_num]) / len(bow_corpus) \\\n",
    "                      for topic_num in significant_topics]\n",
    "\n",
    "    topic_indexes_by_avg_coherence = [tup[0] for tup in sorted(enumerate(topic_averages), key=lambda i:i[1])[::-1]]\n",
    "\n",
    "    significant_topics_by_avg_coherence = [significant_topics[i] for i in topic_indexes_by_avg_coherence]\n",
    "    ordered_topics = [model_topics[i] for i in significant_topics_by_avg_coherence][:num_topics] # limit for HDP\n",
    "\n",
    "    ordered_topic_averages = [topic_averages[i] for i in topic_indexes_by_avg_coherence][:num_topics] # limit for HDP\n",
    "    ordered_topic_averages = [a/sum(ordered_topic_averages) for a in ordered_topic_averages] # normalize HDP values\n",
    "\n",
    "    return ordered_topics, ordered_topic_averages\n",
    "\n",
    "ordered_topics, ordered_topic_averages = \\\n",
    "    order_subset_by_coherence(dirichlet_model=dirichlet_model,\n",
    "                              bow_corpus=bow_corpus, \n",
    "                              num_topics=num_topics,\n",
    "                              num_keywords=num_keywords)\n",
    "\n",
    "keywords = []\n",
    "for i in range(num_topics):\n",
    "    # Find the number of indexes to select, which can later be extended if the word has already been selected\n",
    "    selection_indexes = list(range(int(round(num_keywords * ordered_topic_averages[i]))))\n",
    "    if selection_indexes == [] and len(keywords) < num_keywords: \n",
    "        # Fix potential rounding error by giving this topic one selection\n",
    "        selection_indexes = [0]\n",
    "              \n",
    "    for s_i in selection_indexes:\n",
    "        if ordered_topics[i][s_i] not in keywords and ordered_topics[i][s_i] not in ignore_words:\n",
    "            keywords.append(ordered_topics[i][s_i])\n",
    "        else:\n",
    "            selection_indexes.append(selection_indexes[-1] + 1)\n",
    "\n",
    "# Fix for if too many were selected\n",
    "keywords = keywords[:num_keywords]\n",
    "\n",
    "print(time.perf_counter()-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
